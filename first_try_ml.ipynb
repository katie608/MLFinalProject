{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first-try-ml.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KbrKo8jWMNn",
        "colab_type": "text"
      },
      "source": [
        "## ML Final Project first try of making an ML model\n",
        "\n",
        "Use ngrams code from sentiment analysis to make a fist pass at making a ML model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGfQ2A7jkJUO",
        "colab_type": "text"
      },
      "source": [
        "First, I did a lot of pre-processing that got the data into a usable form. You can see the code for pre-processing here: https://colab.research.google.com/drive/1IVmYII4bwYTvOjXig5POtmhTEMWgaBC3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTUjkvrNV1_W",
        "colab_type": "code",
        "outputId": "a1b73d8a-d6f3-49ff-c16a-390c1b94a725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "# Import data (for the code that produced this data, see notebook: https://colab.research.google.com/drive/1IVmYII4bwYTvOjXig5POtmhTEMWgaBC3)\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# https://drive.google.com/open?id=10Te32ZdtwxPbKdqd5YZw6Ztoa4g7FYDB\n",
        "gdown.download('https://drive.google.com/uc?authuser=0&id=10Te32ZdtwxPbKdqd5YZw6Ztoa4g7FYDB&export=download',\n",
        "               'finalprojectdata.csv',\n",
        "               quiet=False)\n",
        "df = pd.read_csv('finalprojectdata.csv', header=0, delimiter=',')\n",
        "df"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?authuser=0&id=10Te32ZdtwxPbKdqd5YZw6Ztoa4g7FYDB&export=download\n",
            "To: /content/finalprojectdata.csv\n",
            "100%|██████████| 344k/344k [00:00<00:00, 47.3MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1_0</td>\n",
              "      <td>0</td>\n",
              "      <td>find out more here</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2_0</td>\n",
              "      <td>0</td>\n",
              "      <td>i had a long battle with anorexia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3_0</td>\n",
              "      <td>0</td>\n",
              "      <td>those thoughts telling me that if i just lost...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4_0</td>\n",
              "      <td>0</td>\n",
              "      <td>the trouble is that never happened</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5_0</td>\n",
              "      <td>0</td>\n",
              "      <td>there was never a magic number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3364</th>\n",
              "      <td>2131_1</td>\n",
              "      <td>1</td>\n",
              "      <td>the last pro ana diet comes with a twist in at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3365</th>\n",
              "      <td>2132_1</td>\n",
              "      <td>1</td>\n",
              "      <td>in this diet, you can hardly eat any carbs bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3366</th>\n",
              "      <td>2133_1</td>\n",
              "      <td>1</td>\n",
              "      <td>with this diet, you will see a drastic loss i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3367</th>\n",
              "      <td>2134_1</td>\n",
              "      <td>1</td>\n",
              "      <td>well, these were some of the best pro ana diet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3368</th>\n",
              "      <td>2135_1</td>\n",
              "      <td>1</td>\n",
              "      <td>everyone aims to have a thin body and these d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3369 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  label                                               text\n",
              "0        1_0      0                                 find out more here\n",
              "1        2_0      0                  i had a long battle with anorexia\n",
              "2        3_0      0   those thoughts telling me that if i just lost...\n",
              "3        4_0      0                 the trouble is that never happened\n",
              "4        5_0      0                     there was never a magic number\n",
              "...      ...    ...                                                ...\n",
              "3364  2131_1      1  the last pro ana diet comes with a twist in at...\n",
              "3365  2132_1      1   in this diet, you can hardly eat any carbs bu...\n",
              "3366  2133_1      1   with this diet, you will see a drastic loss i...\n",
              "3367  2134_1      1  well, these were some of the best pro ana diet...\n",
              "3368  2135_1      1   everyone aims to have a thin body and these d...\n",
              "\n",
              "[3369 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QYpM8qiXFDf",
        "colab_type": "text"
      },
      "source": [
        "There are a lot of weird symbols that come from having a list of strings, but they will all be removed when we clean the data. \n",
        "\n",
        "Then, convert from text to a bag of words representation using scikit learn's built-in [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ0UE8IoW8KN",
        "colab_type": "code",
        "outputId": "6096264b-bc48-4dbc-ab42-cabc33cfa04e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = np.array(df['label'])\n",
        "\n",
        "dfX_train, dfX_test, y_train, y_test = train_test_split(df['text'], y)\n",
        "print(\"df_train.shape\",dfX_train.shape)\n",
        "print(\"y_train.shape\",y_train.shape)\n",
        "print(\"dfX_test.shape\",dfX_test.shape)\n",
        "print(\"y_test.shape\",y_test.shape)\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, min_df = 20) #convert a collection of text documents into a matrix of token counts\n",
        "vectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
        "\n",
        "X_train = vectorizer.transform(dfX_train).todense() #transform to a document-term matrix\n",
        "X_test = vectorizer.transform(dfX_test).todense()\n",
        "print(\"X_test.shape\",X_test.shape)\n",
        "print(\"X_train.shape\", X_train.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_train.shape (2526,)\n",
            "y_train.shape (2526,)\n",
            "dfX_test.shape (843,)\n",
            "y_test.shape (843,)\n",
            "X_test.shape (843, 260)\n",
            "X_train.shape (2526, 260)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ed5j_vlr-Cu",
        "colab_type": "text"
      },
      "source": [
        "We also split the data into training and test right from the start. We'll check to make sure our data is organized properly. The word \"king\" should occur more frequently in Grimm's Fairy Tales, and the word \"sherlock\" should only appear in The Adventures of Sherlock Holmes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYaufLj5g65W",
        "colab_type": "code",
        "outputId": "762d5945-9fa2-4cbd-e697-2d84196505e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Looking at a paragraph to make sure things work\n",
        "reviews_wrapped = dfX_train.str.wrap(80)\n",
        "calories_index = vectorizer.get_feature_names().index('calories')\n",
        "print(\"calories occurs in\", X_train[y_train==1, calories_index].mean(), \"for Y=1\")\n",
        "print(\"calories occurs in\", X_train[y_train==0, calories_index].mean(), \"for Y=0\")\n",
        "print(reviews_wrapped.iloc[1]) # Just in case you want to read a random paragraph"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calories occurs in 0.05040957781978576 for Y=1\n",
            "calories occurs in 0.004259850905218318 for Y=0\n",
            " the first wave of pro-ana websites was observed in the 1990s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7esYTmjpfqgr",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the Parameters of the Model & Making Predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvXgkDmK60vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_nb_model(X, y):\n",
        "    X_1 = np.asarray(X[y == 1, :]) # all paragraphs from Sherlock\n",
        "    X_0 = np.asarray(X[y == 0, :]) # all paragraphs from Grimm\n",
        "    return y.mean(), 1 - y.mean(), X_1.mean(axis=0), X_0.mean(axis=0)\n",
        "\n",
        "def get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X):\n",
        "    \"\"\" Predict the labels for the data X given the Naive Bayes model \"\"\"\n",
        "    log_odds_ratios = np.zeros(X.shape[0])\n",
        "    for i in range(X.shape[0]): # loop over data points\n",
        "        if i%(X.shape[0]/10) == 0: print(\"progress\", i/X.shape[0])\n",
        "        log_odds_ratios[i] += np.log(p_y_1) - np.log(p_y_0)\n",
        "        for j in range(X.shape[1]): #loop over words\n",
        "            if X[i, j] == 1: #if this example includes word j\n",
        "                log_odds_ratios[i] += np.log(p_x_y_1[j]) - np.log(p_x_y_0[j])\n",
        "            else: \n",
        "                log_odds_ratios[i] += np.log(1 - p_x_y_1[j]) - np.log(1 - p_x_y_0[j])\n",
        "    return (log_odds_ratios >= 0).astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJn8MvlckybG",
        "colab_type": "code",
        "outputId": "3f9b5901-4c6d-4714-be98-7020753c0993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB(alpha=1) \n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test[:100,:])\n",
        "np.mean(y_pred == y_test[:100])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS0fO4gcugQ4",
        "colab_type": "text"
      },
      "source": [
        "### Model including Laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YgnBMrdyMso",
        "colab_type": "code",
        "outputId": "f5bc9c53-1918-49b0-ab98-bf74e22abb39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "def fit_nb_model_smooth(X, y,alpha):\n",
        "    X_1 = np.asarray(X[y == 1, :])\n",
        "    X_0 = np.asarray(X[y == 0, :])\n",
        "    N_1,V_1 = X_1.shape\n",
        "    N_0,V_0 = X_0.shape #should actually be the same size in our case\n",
        "    return y.mean(), 1 - y.mean(), np.divide(X_1.sum(axis=0)+alpha,N_1), np.divide(X_0.sum(axis=0)+1,N_0) \n",
        "\n",
        "\n",
        "# Code to call and run your new fitting with alpha =1\n",
        "p_y_1, p_y_0, p_x_y_1, p_x_y_0 = fit_nb_model_smooth(X_train, y_train,1) #Model with smoothing\n",
        "y_pred = get_nb_predictions(p_y_1, p_y_0, p_x_y_1, p_x_y_0, X_test[:100,:]) #Only looking at first 100 X_test\n",
        "print(\"accuracy is\", (y_pred == y_test[:100]).astype(np.float).mean()) #also only need to compare first 100 y_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 0.0\n",
            "progress 0.1\n",
            "progress 0.2\n",
            "progress 0.3\n",
            "progress 0.4\n",
            "progress 0.5\n",
            "progress 0.6\n",
            "progress 0.7\n",
            "progress 0.8\n",
            "progress 0.9\n",
            "accuracy is 0.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x08auTXzqD2",
        "colab_type": "code",
        "outputId": "2fc39131-c164-43d0-9ae1-73483c803a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X_0 = np.asarray(X_train[y_train == 0, :])\n",
        "print(np.shape(X_0))\n",
        "print(np.shape(p_x_y_0))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(939, 260)\n",
            "(260,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEvVQAyJ2vIG",
        "colab_type": "text"
      },
      "source": [
        "### Clean Text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZMbLwn7jY-",
        "colab_type": "code",
        "outputId": "fcc68b12-4b93-4362-92bf-6a9beaacdbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "import collections\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import re\n",
        "# Essentially the same as above, but putting it into a function for later\n",
        "def clean_text(s):\n",
        "  s = s.lower() # Convert to lowercases\n",
        "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # Replace all non alphanumeric characters with spaces\n",
        "  s = re.sub(' +',' ',s) # Replace series of spaces with single space\n",
        "  return s\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  origtext = list(dfX_train)[i]\n",
        "  print(\"Original text: \",origtext)\n",
        "  cleaned = clean_text(origtext)\n",
        "  print(\"Cleaned  text: \",cleaned)\n",
        "  tokens = [token for token in cleaned.split(\" \") if token != \"\"]\n",
        "  bigramWords = list(ngrams(tokens, 2))\n",
        "  bigramFreq = collections.Counter(bigramWords)\n",
        "  print(\"Bigrams: \", bigramFreq.most_common(10))\n",
        "  print('\\n')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:  through acceptance and commitment therapy, a person with an eating disorder can learn to break destructive cycles of negative thoughts\n",
            "Cleaned  text:  through acceptance and commitment therapy a person with an eating disorder can learn to break destructive cycles of negative thoughts\n",
            "Bigrams:  [(('through', 'acceptance'), 1), (('acceptance', 'and'), 1), (('and', 'commitment'), 1), (('commitment', 'therapy'), 1), (('therapy', 'a'), 1), (('a', 'person'), 1), (('person', 'with'), 1), (('with', 'an'), 1), (('an', 'eating'), 1), (('eating', 'disorder'), 1)]\n",
            "\n",
            "\n",
            "Original text:   the first wave of pro-ana websites was observed in the 1990s\n",
            "Cleaned  text:   the first wave of pro ana websites was observed in the 1990s\n",
            "Bigrams:  [(('the', 'first'), 1), (('first', 'wave'), 1), (('wave', 'of'), 1), (('of', 'pro'), 1), (('pro', 'ana'), 1), (('ana', 'websites'), 1), (('websites', 'was'), 1), (('was', 'observed'), 1), (('observed', 'in'), 1), (('in', 'the'), 1)]\n",
            "\n",
            "\n",
            "Original text:  if there’s anything you guys would like to see, please let me know! i’ll be glad to add it\n",
            "Cleaned  text:  if there s anything you guys would like to see please let me know i ll be glad to add it\n",
            "Bigrams:  [(('if', 'there'), 1), (('there', 's'), 1), (('s', 'anything'), 1), (('anything', 'you'), 1), (('you', 'guys'), 1), (('guys', 'would'), 1), (('would', 'like'), 1), (('like', 'to'), 1), (('to', 'see'), 1), (('see', 'please'), 1)]\n",
            "\n",
            "\n",
            "Original text:  poshmark is a buying and selling app and website! you can find new and preloved items at a fraction of the cost! it’s a great way to save money and save the earth! if you didn’t know, the amount of water it takes to make one pair of jeans could provide water for someone for 900 days\n",
            "Cleaned  text:  poshmark is a buying and selling app and website you can find new and preloved items at a fraction of the cost it s a great way to save money and save the earth if you didn t know the amount of water it takes to make one pair of jeans could provide water for someone for 900 days\n",
            "Bigrams:  [(('poshmark', 'is'), 1), (('is', 'a'), 1), (('a', 'buying'), 1), (('buying', 'and'), 1), (('and', 'selling'), 1), (('selling', 'app'), 1), (('app', 'and'), 1), (('and', 'website'), 1), (('website', 'you'), 1), (('you', 'can'), 1)]\n",
            "\n",
            "\n",
            "Original text:  the holiday season is now in full swing which means many gatherings involving substantial meals\n",
            "Cleaned  text:  the holiday season is now in full swing which means many gatherings involving substantial meals\n",
            "Bigrams:  [(('the', 'holiday'), 1), (('holiday', 'season'), 1), (('season', 'is'), 1), (('is', 'now'), 1), (('now', 'in'), 1), (('in', 'full'), 1), (('full', 'swing'), 1), (('swing', 'which'), 1), (('which', 'means'), 1), (('means', 'many'), 1)]\n",
            "\n",
            "\n",
            "Original text:   pro ana diet has now become a lifestyle who also call themselves as the member of pro ana community\n",
            "Cleaned  text:   pro ana diet has now become a lifestyle who also call themselves as the member of pro ana community\n",
            "Bigrams:  [(('pro', 'ana'), 2), (('ana', 'diet'), 1), (('diet', 'has'), 1), (('has', 'now'), 1), (('now', 'become'), 1), (('become', 'a'), 1), (('a', 'lifestyle'), 1), (('lifestyle', 'who'), 1), (('who', 'also'), 1), (('also', 'call'), 1)]\n",
            "\n",
            "\n",
            "Original text:  find that you don't need caffeine after the third day\n",
            "Cleaned  text:  find that you don t need caffeine after the third day\n",
            "Bigrams:  [(('find', 'that'), 1), (('that', 'you'), 1), (('you', 'don'), 1), (('don', 't'), 1), (('t', 'need'), 1), (('need', 'caffeine'), 1), (('caffeine', 'after'), 1), (('after', 'the'), 1), (('the', 'third'), 1), (('third', 'day'), 1)]\n",
            "\n",
            "\n",
            "Original text:  5 cals)lunch: ½ cup strawberries (21\n",
            "Cleaned  text:  5 cals lunch cup strawberries 21\n",
            "Bigrams:  [(('5', 'cals'), 1), (('cals', 'lunch'), 1), (('lunch', 'cup'), 1), (('cup', 'strawberries'), 1), (('strawberries', '21'), 1)]\n",
            "\n",
            "\n",
            "Original text:  treatment and recovery from an eating disorder can be expensive and time-consuming\n",
            "Cleaned  text:  treatment and recovery from an eating disorder can be expensive and time consuming\n",
            "Bigrams:  [(('treatment', 'and'), 1), (('and', 'recovery'), 1), (('recovery', 'from'), 1), (('from', 'an'), 1), (('an', 'eating'), 1), (('eating', 'disorder'), 1), (('disorder', 'can'), 1), (('can', 'be'), 1), (('be', 'expensive'), 1), (('expensive', 'and'), 1)]\n",
            "\n",
            "\n",
            "Original text:   pinching for fatness, continually weighing yourself, or trying on too-small clothes only magnifies a negative self-view and gives you a distorted image of what you really look like\n",
            "Cleaned  text:   pinching for fatness continually weighing yourself or trying on too small clothes only magnifies a negative self view and gives you a distorted image of what you really look like\n",
            "Bigrams:  [(('pinching', 'for'), 1), (('for', 'fatness'), 1), (('fatness', 'continually'), 1), (('continually', 'weighing'), 1), (('weighing', 'yourself'), 1), (('yourself', 'or'), 1), (('or', 'trying'), 1), (('trying', 'on'), 1), (('on', 'too'), 1), (('too', 'small'), 1)]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsh1wWW07g4i",
        "colab_type": "text"
      },
      "source": [
        "#### Now, let's clean all the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JMfLVyry0z4",
        "colab_type": "code",
        "outputId": "dc89d78c-c077-47a8-d734-482bbec3a808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def clean_series_data(sdata):\n",
        "  sdata = list(sdata)\n",
        "  for i in range(len(sdata)):\n",
        "    sdata[i] = clean_text(sdata[i])\n",
        "    if i%(len(sdata)/5)==0:\n",
        "      print(sdata[i]) #Printing occasional text can be helpful for making sure that your cleaning is working how you want it to. Or you can comment this out.\n",
        "  return sdata\n",
        "\n",
        "dfX_train = clean_series_data(dfX_train)\n",
        "dfX_test = clean_series_data(dfX_test)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "through acceptance and commitment therapy a person with an eating disorder can learn to break destructive cycles of negative thoughts\n",
            " you need to believe that now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGDdKfQP1lv0",
        "colab_type": "text"
      },
      "source": [
        "###Notebook Exercise 3 \n",
        "\n",
        "Find the top 10 bigrams for Sherlock and Grimm paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFkxMjSy0SwT",
        "colab_type": "code",
        "outputId": "64e05923-9994-47cc-fdce-e9e09a8ce125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# A solution (though there are likely faster, cleaner ways to do this)\n",
        "wordsProAna = list() # initialize empty list for words\n",
        "wordsProRecovery = list() # initialize empty list for words\n",
        "n = 4\n",
        "\n",
        "for i in range(len(y_train)): # iterate over y_train\n",
        "  # split words?\n",
        "  tokens = [token for token in dfX_train[i].split(\" \") if token != \"\"] \n",
        "  if y_train[i]==1: # if y_train[i] is Sherlock\n",
        "    wordsProAna.extend(tokens) # add list of tokens to wordssherlock list\n",
        "  else:\n",
        "    wordsProRecovery.extend(tokens) # add list of tokens to wordsGrimm list\n",
        "\n",
        "# uses nltk library to make ngrams from wordsSherlock, then puts ngrams in list\n",
        "bigramWordsProAna = list(ngrams(wordsProAna, n)) \n",
        "# counts the bigramWordsSherlock list\n",
        "bigramFreqProAna = collections.Counter(bigramWordsProAna)\n",
        "# make ngrams from wordsGrimm and put ngrams in list\n",
        "bigramWordsProRecovery = list(ngrams(wordsProRecovery, n))\n",
        "# counts the bigramWordsGrimm list\n",
        "bigramFreqProRecovery = collections.Counter(bigramWordsProRecovery)\n",
        "\n",
        "print(\"pro-ana bigrams:\")\n",
        "bgfp = bigramFreqProAna.most_common(30) # sets variable bgfp to 10 most common bigrams\n",
        "for bg in bgfp: # iterates over bigrams\n",
        "  print(bg) # prints bigrams\n",
        "print('\\n')\n",
        "print(\"pro-recovery bigrams\")\n",
        "bgfp = bigramFreqProRecovery.most_common(30)\n",
        "for bg in bgfp:\n",
        "  print(bg)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pro-ana bigrams:\n",
            "(('the', 'pro', 'ana', 'diet'), 9)\n",
            "(('plan', 'you', 'have', 'to'), 8)\n",
            "(('pro', 'ana', 'tips', 'and'), 7)\n",
            "(('pro', 'ana', 'diet', 'plan'), 6)\n",
            "(('ana', 'tips', 'and', 'tricks'), 6)\n",
            "(('pro', 'ana', 'diet', 'plans'), 5)\n",
            "(('stay', 'safe', 'stay', 'strong'), 5)\n",
            "(('safe', 'stay', 'strong', 'stay'), 5)\n",
            "(('stay', 'strong', 'stay', 'skinny'), 5)\n",
            "(('pro', 'ana', 'is', 'a'), 5)\n",
            "(('the', 'pro', 'ana', 'lifestyle'), 5)\n",
            "(('this', 'diet', 'plan', 'you'), 5)\n",
            "(('let', 'us', 'begin', 'with'), 5)\n",
            "(('if', 'you', 'want', 'to'), 5)\n",
            "(('diet', 'plan', 'you', 'have'), 5)\n",
            "(('tips', 'and', 'tricks', 'for'), 5)\n",
            "(('that', 'you', 'do', 'not'), 5)\n",
            "(('in', 'front', 'of', 'the'), 5)\n",
            "(('that', 'you', 'don', 't'), 4)\n",
            "(('don', 't', 'want', 'to'), 4)\n",
            "(('you', 'don', 't', 'want'), 4)\n",
            "(('pro', 'ana', 'tips', 'that'), 4)\n",
            "(('the', 'food', 'that', 'you'), 4)\n",
            "(('want', 'to', 'lose', 'weight'), 4)\n",
            "(('day', 'of', 'the', 'cycle'), 4)\n",
            "(('it', 'will', 'make', 'you'), 4)\n",
            "(('you', 'want', 'to', 'eat'), 4)\n",
            "(('this', 'will', 'help', 'you'), 4)\n",
            "(('tips', 'and', 'tricks', 'that'), 4)\n",
            "(('in', 'this', 'diet', 'plan'), 4)\n",
            "\n",
            "\n",
            "pro-recovery bigrams\n",
            "(('from', 'an', 'eating', 'disorder'), 25)\n",
            "(('with', 'an', 'eating', 'disorder'), 14)\n",
            "(('recovery', 'from', 'an', 'eating'), 12)\n",
            "(('value', 'is', 'not', 'determined'), 7)\n",
            "(('is', 'not', 'determined', 'by'), 7)\n",
            "(('not', 'determined', 'by', 'appearance'), 7)\n",
            "(('an', 'eating', 'disorder', 'is'), 6)\n",
            "(('in', 'the', 'treatment', 'of'), 6)\n",
            "(('it', 'is', 'important', 'to'), 6)\n",
            "(('my', 'value', 'is', 'not'), 6)\n",
            "(('a', 'person', 'with', 'an'), 5)\n",
            "(('person', 'with', 'an', 'eating'), 5)\n",
            "(('to', 'learn', 'more', 'about'), 5)\n",
            "(('people', 'with', 'eating', 'disorders'), 5)\n",
            "(('struggling', 'with', 'an', 'eating'), 5)\n",
            "(('recovering', 'from', 'an', 'eating'), 5)\n",
            "(('at', 'the', 'emily', 'program'), 5)\n",
            "(('the', 'treatment', 'of', 'eating'), 5)\n",
            "(('treatment', 'of', 'eating', 'disorders'), 5)\n",
            "(('don', 't', 'want', 'to'), 5)\n",
            "(('it', 's', 'important', 'to'), 4)\n",
            "(('when', 'it', 'comes', 'to'), 4)\n",
            "(('an', 'eating', 'disorder', 'i'), 4)\n",
            "(('in', 'the', 'recovery', 'process'), 4)\n",
            "(('the', 'road', 'to', 'recovery'), 4)\n",
            "(('about', 'your', 'eating', 'disorder'), 4)\n",
            "(('an', 'eating', 'disorder', 'can'), 3)\n",
            "(('not', 'be', 'able', 'to'), 3)\n",
            "(('eating', 'disorder', 'support', 'group'), 3)\n",
            "(('learn', 'more', 'about', 'how'), 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKZmbjRT7-hS",
        "colab_type": "text"
      },
      "source": [
        "#### Try a different sized n-gram (instead of a bigram / 2-gram)\n",
        "Not shockingly, this list is not very exciting. It turns out, people use some pretty standard words bigrams when talking about movies (e.g. \"this film\" and \"of the\")... really riveting stuff. If we look at a greater number of bigrams (e.g., the top 100), we can eventually start to find something relevant among mostly trite pairings.\n",
        "\n",
        "However, it might be interesting to look at a different sized ngram than the bigram. **Try something in the n = 4 to 7 range.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjeDdSF6FZsL",
        "colab_type": "text"
      },
      "source": [
        "## Classifying movie review sentiment with bigrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSEKwuOKncmZ",
        "colab_type": "text"
      },
      "source": [
        "Let's revisit our Na&iuml;ve Bayes model, but now using bigrams as our features instead of single words. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6BU3Z-g-Gcs",
        "colab_type": "text"
      },
      "source": [
        "#### Use CountVectorizer to get top bigrams and then classify sentiment\n",
        "The code below gives a black box approach to classifying with ngrams. \n",
        "\n",
        "The ngram_range(2,2) makes our code use bigrams. \n",
        "\n",
        "Note that we now have a different shape to our data because it is stored in sparse form (no longer using todense()). If we try to store this in dense form, we will run into RAM errors, which we could combat by limiting the number of ngrams that we include in our CountVectorizer by setting max_features=10000 limits the total number of feautures.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBxljA3JZsKn",
        "colab_type": "code",
        "outputId": "8029c4e6-acea-4e9c-b88a-587460f697c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "ngramvectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "ngramvectorizer.fit(dfX_train) #learn a vocabulary dictionary of all tokens in the raw documents\n",
        "\n",
        "X_train_ngram = ngramvectorizer.transform(dfX_train)\n",
        "X_test_ngram = ngramvectorizer.transform(dfX_test)\n",
        "print(\"X_train_ngram.shape\", X_train_ngram.shape)\n",
        "print(\"X_test_ngram.shape\", X_test_ngram.shape) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_ngram.shape (2526, 23340)\n",
            "X_test_ngram.shape (843, 23340)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KNyelzfhMcK",
        "colab_type": "code",
        "outputId": "2a0e4b79-b01d-4633-9223-3c7340d25ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Actually run the model and print results\n",
        "model = MultinomialNB(alpha=1)\n",
        "model.fit(X_train_ngram, y_train)\n",
        "\n",
        "y_pred_train = model.predict(X_train_ngram)\n",
        "print(\"Training accuracy: \", np.mean(y_pred_train == y_train))\n",
        "y_pred = model.predict(X_test_ngram)\n",
        "print(\"Testing accuracy: \",np.mean(y_pred == y_test))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy:  0.9901029295328583\n",
            "Testing accuracy:  0.8327402135231317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSUIpaW5EJi8",
        "colab_type": "text"
      },
      "source": [
        "## Predicting the next words with bigrams\n",
        "Use bigrams to generate a predicted sequence of words when given a single word. I modified this code to be a function which I then used several times with different texts and different starting words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fsbbASsUOAV",
        "colab_type": "code",
        "outputId": "20a4b661-e0e6-49e9-ff3c-6b219c6aa89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import random\n",
        "def generate_text(bigramList, startingWord):\n",
        "  bigramLookup = {}\n",
        "\n",
        "  for i in range(len(bigramList)-1):\n",
        "      w1 = bigramList[i][0]\n",
        "      w2 = bigramList[i][1]\n",
        "      #print(w1,w2)\n",
        "      if  w1 not in bigramLookup.keys():\n",
        "        bigramLookup[w1] = {w2:1}\n",
        "      elif w2 not in bigramLookup[w1].keys():\n",
        "        bigramLookup[w1][w2] = 1\n",
        "      else:\n",
        "        bigramLookup[w1][w2] = bigramLookup[w1][w2] + 1\n",
        "\n",
        "  curr_sequence = startingWord \n",
        "  output = curr_sequence\n",
        "  for i in range(50):\n",
        "      if curr_sequence not in bigramLookup.keys():\n",
        "        print(\"not in my keys, choosing seed word \")\n",
        "        output += '. '\n",
        "        curr_sequence = 'the'\n",
        "        output += curr_sequence\n",
        "      else: \n",
        "        possible_words = list(bigramLookup[curr_sequence].keys())\n",
        "        next_word = possible_words[random.randrange(len(possible_words))] #Randomly choose a word\n",
        "        output += ' ' + next_word\n",
        "        curr_sequence = next_word\n",
        "        \n",
        "\n",
        "  print(output)\n",
        "\n",
        "generate_text(bigramWordsProAna, \"food\")\n",
        "generate_text(bigramWordsProRecovery, \"food\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "food many bites making sure that thinspiration pictures to weight please don t use stored food that makes me know before eating almost half the babies 200 7 keep reading to some inspirational lyrics such rewards such a charm indicates that then gained  pro ana songs16 pro anna individuals see people\n",
            "food plan or push aside deep feelings food groups 2000 15 8 577 590 your information will to create increased anxiety as psychiatrists psychotherapists and beginning to understand what your treatment options here the risk period in here at camp that rang so hard to realise that no question that take\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}